{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理與文件探勘 HW2\n",
    "\n",
    "# 工作分配\n",
    "\n",
    "| 人員             | 任務                      | 佔比 |\n",
    "| ---------------- | ------------------------- | ---- |\n",
    "| ChatGPT          | 撰寫程式碼                | 1%   |\n",
    "| 111590012 林品緯 | 複製貼上、debug、撰寫文件 | 99%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安裝必要模組\n",
    "\n",
    "**gensim 和 numpy 可能會有版本問題，我的 gensim 為 (4.3.3) numpy 為 (1.24.0)，如果發生衝突，請自行解決。**\n",
    "\n",
    "在開始之前，請先安裝以下必要的 Python 模組。你可以使用以下指令安裝："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joseph\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joseph\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\joseph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim pandas scipy nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word1     Word2  Similarity\n",
       "0      love       sex        6.77\n",
       "1     tiger       cat        7.35\n",
       "2     tiger     tiger       10.00\n",
       "3      book     paper        7.46\n",
       "4  computer  keyboard        7.62"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 設定當前工作目錄\n",
    "current_dir = Path().resolve()\n",
    "\n",
    "# 解壓縮文件\n",
    "zip_file_path = current_dir / \"wordsim353.zip\"\n",
    "extract_to = current_dir / \"wordsim353\" \n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "# 讀取 CSV 文件\n",
    "wordsim_df = pd.read_csv(extract_to / \"combined.csv\")\n",
    "wordsim_df.columns = ['Word1', 'Word2', 'Similarity']\n",
    "\n",
    "zip_file_path = current_dir / \"BATS_3.0.zip\"\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(current_dir)\n",
    "\n",
    "bats_dir = current_dir / \"BATS_3.0\"\n",
    "\n",
    "wordsim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 gensim 訓練 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.downloader import load\n",
    "\n",
    "corpus = load(\"text8\")\n",
    "\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=5, workers=4)\n",
    "model.save(\"word2vec_text8.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordSim-353 相似度評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.6249317685509049\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def get_similarity(w1, w2, model):\n",
    "    try:\n",
    "        return model.wv.similarity(w1.lower(), w2.lower())\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "pred_scores = []\n",
    "true_scores = []\n",
    "\n",
    "for _, row in wordsim_df.iterrows():\n",
    "    sim = get_similarity(row['Word1'], row['Word2'], model)\n",
    "    if sim is not None:\n",
    "        pred_scores.append(sim)\n",
    "        true_scores.append(row['Similarity'])\n",
    "\n",
    "correlation, _ = spearmanr(pred_scores, true_scores)\n",
    "print(\"Spearman correlation:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATS 類比預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Inflectional_morphology/I01 [noun - plural_reg].txt: 1514/2450 = 61.80%\n",
      "1_Inflectional_morphology/I02 [noun - plural_irreg].txt: 931/2450 = 38.00%\n",
      "1_Inflectional_morphology/I03 [adj - comparative].txt: 52/931 = 5.59%\n",
      "1_Inflectional_morphology/I04 [adj - superlative].txt: 4/931 = 0.43%\n",
      "1_Inflectional_morphology/I05 [verb_inf - 3pSg].txt: 1471/2450 = 60.04%\n",
      "1_Inflectional_morphology/I06 [verb_inf - Ving].txt: 1164/2450 = 47.51%\n",
      "1_Inflectional_morphology/I07 [verb_inf - Ved].txt: 930/2450 = 37.96%\n",
      "1_Inflectional_morphology/I08 [verb_Ving - 3pSg].txt: 1050/2401 = 43.73%\n",
      "1_Inflectional_morphology/I09 [verb_Ving - Ved].txt: 866/2450 = 35.35%\n",
      "1_Inflectional_morphology/I10 [verb_3pSg - Ved].txt: 837/2450 = 34.16%\n",
      "2_Derivational_morphology/D01 [noun+less_reg].txt: 0/784 = 0.00%\n",
      "2_Derivational_morphology/D02 [un+adj_reg].txt: 181/2401 = 7.54%\n",
      "2_Derivational_morphology/D03 [adj+ly_reg].txt: 168/2450 = 6.86%\n",
      "2_Derivational_morphology/D04 [over+adj_reg].txt: skipped\n",
      "2_Derivational_morphology/D05 [adj+ness_reg].txt: 8/1127 = 0.71%\n",
      "2_Derivational_morphology/D06 [re+verb_reg].txt: 12/1056 = 1.14%\n",
      "2_Derivational_morphology/D07 [verb+able_reg].txt: 8/1584 = 0.51%\n",
      "2_Derivational_morphology/D08 [verb+er_irreg].txt: 27/1890 = 1.43%\n",
      "2_Derivational_morphology/D09 [verb+tion_irreg].txt: 19/1320 = 1.44%\n",
      "2_Derivational_morphology/D10 [verb+ment_irreg].txt: 87/2256 = 3.86%\n",
      "3_Encyclopedic_semantics/E01 [country - capital].txt: 527/2304 = 22.87%\n",
      "3_Encyclopedic_semantics/E02 [country - language].txt: 126/1470 = 8.57%\n",
      "3_Encyclopedic_semantics/E03 [UK_city - county].txt: 29/2304 = 1.26%\n",
      "3_Encyclopedic_semantics/E04 [name - nationality].txt: 25/1632 = 1.53%\n",
      "3_Encyclopedic_semantics/E05 [name - occupation].txt: 88/864 = 10.19%\n",
      "3_Encyclopedic_semantics/E06 [animal - young].txt: 7/1012 = 0.69%\n",
      "3_Encyclopedic_semantics/E07 [animal - sound].txt: 0/1128 = 0.00%\n",
      "3_Encyclopedic_semantics/E08 [animal - shelter].txt: 0/322 = 0.00%\n",
      "3_Encyclopedic_semantics/E09 [things - color].txt: 38/940 = 4.04%\n",
      "3_Encyclopedic_semantics/E10 [male - female].txt: 271/1666 = 16.27%\n",
      "4_Lexicographic_semantics/L01 [hypernyms - animals].txt: skipped\n",
      "4_Lexicographic_semantics/L02 [hypernyms - misc].txt: skipped\n",
      "4_Lexicographic_semantics/L03 [hyponyms - misc].txt: skipped\n",
      "4_Lexicographic_semantics/L04 [meronyms - substance].txt: 5/423 = 1.18%\n",
      "4_Lexicographic_semantics/L05 [meronyms - member].txt: 7/1008 = 0.69%\n",
      "4_Lexicographic_semantics/L06 [meronyms - part].txt: 0/141 = 0.00%\n",
      "4_Lexicographic_semantics/L07 [synonyms - intensity].txt: 0/135 = 0.00%\n",
      "4_Lexicographic_semantics/L08 [synonyms - exact].txt: 1/245 = 0.41%\n",
      "4_Lexicographic_semantics/L09 [antonyms - gradable].txt: skipped\n",
      "4_Lexicographic_semantics/L10 [antonyms - binary].txt: 25/423 = 5.91%\n",
      "\n",
      "Overall Accuracy: 10478/52298 = 20.04%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model = Word2Vec.load(\"word2vec_text8.model\")\n",
    "\n",
    "def analogy_accuracy_pairwise(file_path, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pairs = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) != 2:\n",
    "                continue\n",
    "            pairs.append(tokens)\n",
    "\n",
    "    for i in range(len(pairs)):\n",
    "        for j in range(len(pairs)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            a, b = pairs[i]\n",
    "            c, d = pairs[j]\n",
    "            try:\n",
    "                predicted = model.wv.most_similar(positive=[b, c], negative=[a], topn=1)[0][0]\n",
    "                if predicted == d:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    return correct, total\n",
    "\n",
    "\n",
    "total_correct = 0\n",
    "total_questions = 0\n",
    "\n",
    "for subfolder in os.listdir(bats_dir):\n",
    "    subfolder_path = os.path.join(bats_dir, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(subfolder_path, file)\n",
    "                c, t = analogy_accuracy_pairwise(file_path, model)\n",
    "                total_correct += c\n",
    "                total_questions += t\n",
    "                print(f\"{subfolder}/{file}: {c}/{t} = {c/t:.2%}\" if t > 0 else f\"{subfolder}/{file}: skipped\")\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {total_correct}/{total_questions} = {total_correct/total_questions:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 和 TF-IDF + SVD 比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF-IDF + SVD from text8] Spearman correlation: 0.0236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "new_corpus = [\" \".join(doc) for doc in corpus]\n",
    "\n",
    "# TF-IDF 建模\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "tfidf_matrix = vectorizer.fit_transform(new_corpus)\n",
    "\n",
    "# SVD 降維\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# 使用 TF-IDF.T × SVD 結果取得詞向量\n",
    "word_embeddings = tfidf_matrix.T @ reduced  # shape: (num_terms, 100)\n",
    "index_to_word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "word_vectors = {\n",
    "    index_to_word[i]: word_embeddings[i]\n",
    "    for i in range(word_embeddings.shape[0])\n",
    "}\n",
    "\n",
    "actual, predicted = [], []\n",
    "\n",
    "for _, row in wordsim_df.iterrows():\n",
    "    w1, w2, score = row[\"Word1\"], row[\"Word2\"], row[\"Similarity\"]\n",
    "    if w1 in word_vectors and w2 in word_vectors:\n",
    "        sim = cosine_similarity(\n",
    "            [word_vectors[w1]], [word_vectors[w2]]\n",
    "        )[0][0]\n",
    "        actual.append(score)\n",
    "        predicted.append(sim)\n",
    "    \n",
    "corr, _ = spearmanr(actual, predicted)\n",
    "print(f\"[TF-IDF + SVD from text8] Spearman correlation: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將詞向量應用於其他任務 (HW1 的情感分類)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料載入和預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Joseph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Joseph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Joseph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# 解壓縮文件\n",
    "zip_file_path = current_dir / \"Sentiment-Analysis-Dataset.zip\"\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(current_dir)\n",
    "\n",
    "# 讀取 CSV 文件\n",
    "file_path = current_dir / \"Sentiment Analysis Dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"UTF-8-SIG\", on_bad_lines='skip')\n",
    "df = df[[\"Sentiment\", \"SentimentText\"]].dropna()\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# 停用詞列表\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # 轉小寫\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\",\n",
    "                  text, flags=re.MULTILINE)  # 移除 URL\n",
    "    text = re.sub(r\"@\\w+|\\#\", \"\", text)  # 移除 @標記 和 #標籤\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # 移除非字母字符\n",
    "    tokens = word_tokenize(text)    \n",
    "    tokens = [word for word in tokens if word not in stop_words]  # 去除停用詞\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "df[\"CleanText\"] = df[\"SentimentText\"].apply(clean_text)\n",
    "\n",
    "# 過濾長度過短的句子並抽樣 50000 筆\n",
    "df = df[df[\"CleanText\"].str.len() > 2].sample(n=50000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.56      0.58      0.57      5069\n",
      "    Positive       0.55      0.54      0.55      4931\n",
      "\n",
      "    accuracy                           0.56     10000\n",
      "   macro avg       0.56      0.56      0.56     10000\n",
      "weighted avg       0.56      0.56      0.56     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "w2v_model = Word2Vec.load(\"word2vec_text8.model\")\n",
    "# 將句子轉為平均詞向量\n",
    "def average_vector(tokens):\n",
    "    vecs = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    if vecs:\n",
    "        return np.mean(vecs, axis=0)\n",
    "    return np.zeros(w2v_model.vector_size)\n",
    "\n",
    "X = np.array([average_vector(tokens) for tokens in df[\"CleanText\"]])\n",
    "y = df[\"Sentiment\"].values\n",
    "\n",
    "# 分割資料並訓練分類器\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 顯示分類結果\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
